var documenterSearchIndex = {"docs":
[{"location":"#InformationTheory","page":"Home","title":"InformationTheory","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for InformationTheory.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#InformationTheory.ShannonEntropy.Hist","page":"Home","title":"InformationTheory.ShannonEntropy.Hist","text":"Hist(; bins::Tuple = (-1,))\n\nA method for calculating Shannon entropy using histograms.\n\nFields\n\nbins::Tuple: A tuple specifying the binning strategy for the histogram.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"#InformationTheory.ShannonEntropy.KSG","page":"Home","title":"InformationTheory.ShannonEntropy.KSG","text":"KSG(; k::Int = 5)\n\nA method for calculating Shannon entropy using the Kozachenko-Leonenko (KSG) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"#InformationTheory.ShannonEntropy.ShannonEntropyMethod","page":"Home","title":"InformationTheory.ShannonEntropy.ShannonEntropyMethod","text":"ShannonEntropyMethod\n\nAn abstract type for different methods of calculating Shannon entropy.\n\n\n\n\n\n","category":"type"},{"location":"#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.Hist, Vararg{AbstractVector}}","page":"Home","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::Hist, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function first fits a histogram to the data x. The probability density function (PDF) is then approximated from the histogram. Finally, the Shannon entropy is calculated by integrating -p(x) * log(p(x)) over the domain of x, where p(x) is the PDF.\n\n\n\n\n\n","category":"method"},{"location":"#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.KSG, Vararg{AbstractVector}}","page":"Home","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::KSG, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using the KSG estimator.\n\nArguments\n\nmethod::KSG: The KSG Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function uses the k-nearest neighbors (k-NN) algorithm to estimate the probability density function (PDF) of the data. The Shannon entropy is then calculated based on the distances to the k-th nearest neighbors. This method is particularly useful for high-dimensional data.\n\n\n\n\n\n","category":"method"}]
}

var documenterSearchIndex = {"docs":
[{"location":"Tutorial/ShannonEntropy/#Shannon-Entropy","page":"Shannon Entropy","title":"Shannon Entropy","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"このページでは，シャノンエントロピーの計算関数shannonについて説明します． この関数は，通常のシャノンエントロピーだけでなく，結合エントロピーも計算可能です．","category":"page"},{"location":"Tutorial/ShannonEntropy/#平均情報量","page":"Shannon Entropy","title":"平均情報量","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"最も簡単な例は，1変数に対するシャノンエントロピー（平均情報量）","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"    H(X) = int_-infty^infty - p(x) ln p(x)  mathrmdx","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"です．","category":"page"},{"location":"Tutorial/ShannonEntropy/#ヒストグラムを使った計算例","page":"Shannon Entropy","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"以下は，ヒストグラム推定を使った計算例です．","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = ShannonEntropy.Hist()\n\n# シャノンエントロピーの推定\nhx = shannon(est, x)\n\n# 結果の出力\nprintln(hx)","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"ヒストグラムを使用する際には，binsを指定できます．","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"est = ShannonEntropy.Hist(bins = (-5:0.25:5,))","category":"page"},{"location":"Tutorial/ShannonEntropy/#k-近傍法を使った計算例","page":"Shannon Entropy","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"k-近傍法は，Kraskov et. al.(2004)が提案した手法に基づいて計算されます． ここでは，","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"    H(X) = -psi(k) + psi(N) + fracdNsum_i=1^N lnepsilon_i","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"で計算します． psiはディガンマ関数で，dは確率変数Xの次元，Nはデータ数です． epsilon_iは各点からk番目に近い点までの距離を表しています． この距離は，NearestNeighbors.jlのkd木を使用して高速に推定されます． 距離推定には論文で提示されている通り，Chebyshev距離を用いています．","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\n\n# KSG推定を使うことを指定\nest = ShannonEntropy.KSG()\n\n# シャノンエントロピーの推定\nhx = shannon(est, x)\n\n# 結果の出力\nprintln(hx)","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"k-近傍法では，kを指定できます．","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"est = ShannonEntropy.KSG(k = 10)","category":"page"},{"location":"Tutorial/ShannonEntropy/#結合エントロピー","page":"Shannon Entropy","title":"結合エントロピー","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"同じ関数を利用して，より高次元の結合エントロピー","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"    H(X Y Z )","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"についても推定可能です．","category":"page"},{"location":"Tutorial/ShannonEntropy/#ヒストグラムを使った計算例-2","page":"Shannon Entropy","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"ここでは，H(XY)を推定する例を示します．","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = ShannonEntropy.Hist()\n\n# シャノンエントロピーの推定\nhx = shannon(est, x, y)\n\n# 結果の出力\nprintln(hx)","category":"page"},{"location":"Tutorial/ShannonEntropy/#k-近傍法を使った計算例-2","page":"Shannon Entropy","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\n\n# KSG推定を使うことを指定\nest = ShannonEntropy.KSG()\n\n# シャノンエントロピーの推定\nhx = shannon(est, x, y)\n\n# 結果の出力\nprintln(hx)","category":"page"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"引数の数（結合数）には制限ありませんが，高次元になるほど推定精度は下がり，推定時間も伸びます．","category":"page"},{"location":"Tutorial/ShannonEntropy/#参考文献","page":"Shannon Entropy","title":"参考文献","text":"","category":"section"},{"location":"Tutorial/ShannonEntropy/","page":"Shannon Entropy","title":"Shannon Entropy","text":"Kraskov A., Stögbauer H., Grassberger P., Estimating mutual information, Phys. Rev. E, Vol. 69 (2004), p. 066138.","category":"page"},{"location":"API/ShannonEntropy/#Shannon-Entropy","page":"Shannon Entropy","title":"Shannon Entropy","text":"","category":"section"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.Hist","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.Hist","text":"Hist(; bins::Tuple = (-1,))\n\nA method for calculating Shannon entropy using histograms.\n\nFields\n\nbins::Tuple: A tuple specifying the binning strategy for the histogram.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.KSG","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.KSG","text":"KSG(; k::Int = 5)\n\nA method for calculating Shannon entropy using the Kozachenko-Leonenko (KSG) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.ShannonEntropyMethod","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.ShannonEntropyMethod","text":"ShannonEntropyMethod\n\nAn abstract type for different methods of calculating Shannon entropy.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.Hist, Vararg{AbstractVector}}","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::Hist, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function first fits a histogram to the data x. The probability density function (PDF) is then approximated from the histogram. Finally, the Shannon entropy is calculated by integrating -p(x) * log(p(x)) over the domain of x, where p(x) is the PDF.\n\n\n\n\n\n","category":"method"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.KSG, Vararg{AbstractVector}}","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::KSG, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using the KSG estimator.\n\nArguments\n\nmethod::KSG: The KSG Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function uses the k-nearest neighbors (k-NN) algorithm to estimate the probability density function (PDF) of the data. The Shannon entropy is then calculated based on the distances to the k-th nearest neighbors. This method is particularly useful for high-dimensional data.\n\n\n\n\n\n","category":"method"},{"location":"Tutorial/ConditionalMutualInformation/#Conditional-Mutual-Information","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"このページでは，条件付き相互情報量 (conditional mutual information) の計算関数c_mutualについて説明します．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#条件付き相互情報量","page":"Conditional Mutual Information","title":"条件付き相互情報量","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"3つの確率変数X Y Zに対する条件付き相互情報量","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"    I(X  Y  Z) = int_-infty^inftyint_-infty^inftyint_-infty^infty p(x y z) log fracp(x y  z)p(x  z) p(y  z)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"です．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#ヒストグラムを使った計算例","page":"Conditional Mutual Information","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"以下は，ヒストグラム推定を使った計算例です．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\nz = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = ConditionalMutualInformation.Hist()\n\n# 条件付き相互情報量の推定\ncmi = c_mutual(est, x, y, z)\n\n# 結果の出力\nprintln(cmi)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"ヒストグラムを使用する際には，bins_x, bins_y, bins_zを指定できます．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"est = ConditionalMutualInformation.Hist(bins_x = (-5:0.25:5,), bins_y = (-5:0.25:5,), bins_z = (-5:0.25:5,))","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#k-近傍法を使った計算例","page":"Conditional Mutual Information","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"k-近傍法は，Kraskov et. al.(2004)が提案した手法に基づいて計算されます． ここでは，","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"    I(X  Y  Z) = psi(k) + langle psi(n_z + 1) - psi(n_xz + 1) - psi(n_yz + 1) rangle","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"で計算します． psiはディガンマ関数で，kは最近傍の数です． n_z n_xz n_yzは、それぞれZ (X Z) (Y Z)空間における最近傍の数を表します。 この距離は，NearestNeighbors.jlのkd木を使用して高速に推定されます． 距離推定には論文で提示されている通り，Chebyshev距離を用いています．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\nz = randn(10000)\n\n# KSG推定を使うことを指定\nest = ConditionalMutualInformation.kNN()\n\n# 条件付き相互情報量の推定\ncmi = c_mutual(est, x, y, z)\n\n# 結果の出力\nprintln(cmi)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"k-近傍法では，kを指定できます．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"est = ConditionalMutualInformation.kNN(k = 10)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#高次元の確率分布","page":"Conditional Mutual Information","title":"高次元の確率分布","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"同じ関数を利用して，より高次元の確率分布に対する条件付き相互情報量も推定可能です． ここでは，I(X_1 X_2  Y_1 Y_2  Z_1 Z_2)を推定する例を示します．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#ヒストグラムを使った計算例-2","page":"Conditional Mutual Information","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\nz1 = randn(10000)\nz2 = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = ConditionalMutualInformation.Hist()\n\n# 条件付き相互情報量の推定\ncmi = c_mutual(est, (x1, x2), (y1, y2), (z1, z2))\n\n# 結果の出力\nprintln(cmi)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#k-近傍法を使った計算例-2","page":"Conditional Mutual Information","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\nz1 = randn(10000)\nz2 = randn(10000)\n\n# KSG推定を使うことを指定\nest = ConditionalMutualInformation.kNN()\n\n# 条件付き相互情報量の推定\ncmi = c_mutual(est, (x1, x2), (y1, y2), (z1, z2))\n\n# 結果の出力\nprintln(cmi)","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"次元の数には制限ありませんが，高次元になるほど推定精度は下がり，推定時間も伸びます．","category":"page"},{"location":"Tutorial/ConditionalMutualInformation/#参考文献","page":"Conditional Mutual Information","title":"参考文献","text":"","category":"section"},{"location":"Tutorial/ConditionalMutualInformation/","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"Kraskov A., Stögbauer H., Grassberger P., Estimating mutual information, Phys. Rev. E, Vol. 69 (2004), p. 066138.","category":"page"},{"location":"API/MutualInformation/#Mutual-Information","page":"Mutual Information","title":"Mutual Information","text":"","category":"section"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.Hist","page":"Mutual Information","title":"InformationTheory.MutualInformation.Hist","text":"Hist(; bins_x::Tuple = (-1,), bins_y::Tuple = (-1,))\n\nA method for calculating mutual information using histograms.\n\nFields\n\nbins_x::Tuple: A tuple specifying the binning strategy for the histogram of x.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\nbins_y::Tuple: A tuple specifying the binning strategy for the histogram of y.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.KSG","page":"Mutual Information","title":"InformationTheory.MutualInformation.KSG","text":"KSG(; k::Int = 5)\n\nA method for calculating mutual information using the Kozachenko-Leonenko-Granger (KSG) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.MutualInformationMethod","page":"Mutual Information","title":"InformationTheory.MutualInformation.MutualInformationMethod","text":"MutualInformationMethod\n\nAn abstract type for different methods of calculating mutual information.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.mutual-Tuple{InformationTheory.MutualInformation.Hist, Tuple, Tuple}","page":"Mutual Information","title":"InformationTheory.MutualInformation.mutual","text":"mutual(method::Hist, x::Tuple, y::Tuple)\n\nCalculates the mutual information between two sets of variables x and y using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based mutual information calculation method.\nx::Tuple: A tuple of vectors representing the data for the first variable.\ny::Tuple: A tuple of vectors representing the data for the second variable.\n\nReturns\n\nI::Float64: The calculated mutual information.\n\nDetails\n\nThe function first fits a histogram to the joint data (x, y). The probability density functions (PDFs) p(x,y), p(x), and p(y) are then approximated from the histogram. Finally, the mutual information is calculated by integrating p(x,y) * log(p(x,y) / (p(x) * p(y))) over the domain of x and y.\n\n\n\n\n\n","category":"method"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.mutual-Tuple{InformationTheory.MutualInformation.KSG, Tuple, Tuple}","page":"Mutual Information","title":"InformationTheory.MutualInformation.mutual","text":"mutual(method::KSG, x::Tuple, y::Tuple)\n\nCalculates the mutual information between two sets of variables x and y using the KSG estimator.\n\nArguments\n\nmethod::KSG: The KSG mutual information calculation method.\nx::Tuple: A tuple of vectors representing the data for the first variable.\ny::Tuple: A tuple of vectors representing the data for the second variable.\n\nReturns\n\nI::Float64: The calculated mutual information.\n\nDetails\n\nThe function uses the k-nearest neighbors (k-NN) algorithm to estimate the mutual information. This method is particularly useful for high-dimensional data. It is based on the work of Kraskov, Stögbauer, and Grassberger (2004).\n\n\n\n\n\n","category":"method"},{"location":"Tutorial/MutualInformation/#Mutual-Information","page":"Mutual Information","title":"Mutual Information","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"このページでは，相互情報量 (mutual information) の計算関数mutualについて説明します．","category":"page"},{"location":"Tutorial/MutualInformation/#相互情報量","page":"Mutual Information","title":"相互情報量","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"2変数に対する相互情報量","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"    I(XY) = int_-infty^infty int_-infty^infty p(x y) ln fracp(x y)p(x)p(y)  mathrmdx mathrmdy","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"です．","category":"page"},{"location":"Tutorial/MutualInformation/#ヒストグラムを使った計算例","page":"Mutual Information","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"以下は，ヒストグラム推定を使った計算例です．","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = MutualInformation.Hist()\n\n# 相互情報量の推定\nmi = mutual(est, x, y)\n\n# 結果の出力\nprintln(mi)","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"ヒストグラムを使用する際には，binsを指定できます．","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"est = MutualInformation.Hist(bins = (-5:0.25:5, -5:0.25:5))","category":"page"},{"location":"Tutorial/MutualInformation/#k-近傍法を使った計算例","page":"Mutual Information","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"k-近傍法は，Kraskov et. al.(2004)が提案した手法に基づいて計算されます． 論文中ではI^(1)とI^(2)が提案されていますが，ここではI^(1)の方法を採用して，","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"    I(XY) = psi(k) + psi(N)- langle psi(n_x + 1) + psi(n_y + 1) rangle","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"で計算します． psiはディガンマ関数で，Nはデータ数です． n_xとn_yは，各点の最近傍距離epsilon_k内に存在する点の数を表します． この距離は，NearestNeighbors.jlのkd木を使用して高速に推定されます． 距離推定には論文で提示されている通り，Chebyshev距離を用いています．","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000)\ny = randn(10000)\n\n# KSG推定を使うことを指定\nest = MutualInformation.KSG()\n\n# 相互情報量の推定\nmi = mutual(est, x, y)\n\n# 結果の出力\nprintln(mi)","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"k-近傍法では，kを指定できます．","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"est = MutualInformation.KSG(k = 10)","category":"page"},{"location":"Tutorial/MutualInformation/#高次元の相互情報量","page":"Mutual Information","title":"高次元の相互情報量","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"より高次元の相互情報量","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"    I(X_1 X_2 dots  Y_1 Y_2 dots)","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"についても推定可能です．","category":"page"},{"location":"Tutorial/MutualInformation/#ヒストグラムを使った計算例-2","page":"Mutual Information","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"ここでは，I(X_1 X_2Y_1Y_2)を推定する例を示します．","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = MutualInformation.Hist()\n\n# 相互情報量の推定\nmi = mutual(est, (x1, x2), (y1, y2))\n\n# 結果の出力\nprintln(mi)","category":"page"},{"location":"Tutorial/MutualInformation/#k-近傍法を使った計算例-2","page":"Mutual Information","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\n\n# KSG推定を使うことを指定\nest = MutualInformation.KSG()\n\n# 相互情報量の推定\nmi = mutual(est, (x1, x2), (y1, y2))\n\n# 結果の出力\nprintln(mi)","category":"page"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"次元の数には制限ありませんが，高次元になるほど推定精度は下がり，推定時間も伸びます．","category":"page"},{"location":"Tutorial/MutualInformation/#参考文献","page":"Mutual Information","title":"参考文献","text":"","category":"section"},{"location":"Tutorial/MutualInformation/","page":"Mutual Information","title":"Mutual Information","text":"Kraskov A., Stögbauer H., Grassberger P., Estimating mutual information, Phys. Rev. E, Vol. 69 (2004), p. 066138.","category":"page"},{"location":"API/ConditionalMutualInformation/#Conditional-Mutual-Information","page":"Conditional Mutual Information","title":"Conditional Mutual Information","text":"","category":"section"},{"location":"API/ConditionalMutualInformation/#InformationTheory.ConditionalMutualInformation.ConditionalMutualInformationMethod","page":"Conditional Mutual Information","title":"InformationTheory.ConditionalMutualInformation.ConditionalMutualInformationMethod","text":"ConditionalMutualInformationMethod\n\nAn abstract type for different methods of calculating conditional mutual information.\n\n\n\n\n\n","category":"type"},{"location":"API/ConditionalMutualInformation/#InformationTheory.ConditionalMutualInformation.Hist","page":"Conditional Mutual Information","title":"InformationTheory.ConditionalMutualInformation.Hist","text":"Hist(; bins_x::Tuple = (-1,), bins_y::Tuple = (-1,), bins_z::Tuple = (-1,))\n\nA method for calculating conditional mutual information using histograms.\n\nFields\n\nbins_x::Tuple: A tuple specifying the binning strategy for the x variable.\nbins_y::Tuple: A tuple specifying the binning strategy for the y variable.\nbins_z::Tuple: A tuple specifying the binning strategy for the z variable. If (-1,) for a variable, the binning is determined automatically by StatsBase.fit. Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/ConditionalMutualInformation/#InformationTheory.ConditionalMutualInformation.kNN","page":"Conditional Mutual Information","title":"InformationTheory.ConditionalMutualInformation.kNN","text":"kNN(; k::Int = 5)\n\nA method for calculating conditional mutual information using the k-Nearest Neighbors (k-NN) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/ConditionalMutualInformation/#InformationTheory.ConditionalMutualInformation.c_mutual-Tuple{InformationTheory.ConditionalMutualInformation.Hist, Tuple, Tuple, Tuple}","page":"Conditional Mutual Information","title":"InformationTheory.ConditionalMutualInformation.c_mutual","text":"c_mutual(method::Hist, x::Tuple, y::Tuple, z::Tuple)\n\nCalculates the conditional mutual information I(X;Y|Z) using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based calculation method.\nx::Tuple: A tuple of vectors representing the data for variable X.\ny::Tuple: A tuple of vectors representing the data for variable Y.\nz::Tuple: A tuple of vectors representing the data for variable Z.\n\nReturns\n\nI::Float64: The calculated conditional mutual information.\n\nDetails\n\nThe function first fits a histogram to the joint data (x, y, z). The probability density function (PDF) is then approximated from the histogram. Finally, the conditional mutual information is calculated from the joint and marginal probabilities. The formula used is: I(X;Y|Z) = sum(p(x,y,z) * log(p(x,y,z) * p(z) / (p(x,z) * p(y,z))))\n\n\n\n\n\n","category":"method"},{"location":"API/ConditionalMutualInformation/#InformationTheory.ConditionalMutualInformation.c_mutual-Tuple{InformationTheory.ConditionalMutualInformation.kNN, Tuple, Tuple, Tuple}","page":"Conditional Mutual Information","title":"InformationTheory.ConditionalMutualInformation.c_mutual","text":"c_mutual(method::kNN, x::Tuple, y::Tuple, z::Tuple)\n\nCalculates the conditional mutual information I(X;Y|Z) using a k-NN based method.\n\nArguments\n\nmethod::kNN: The k-NN based calculation method.\nx::Tuple: A tuple of vectors representing the data for variable X.\ny::Tuple: A tuple of vectors representing the data for variable Y.\nz::Tuple: A tuple of vectors representing the data for variable Z.\n\nReturns\n\nI::Float64: The calculated conditional mutual information.\n\nDetails\n\nThis function uses the k-nearest neighbors (k-NN) algorithm to estimate the conditional mutual information. The estimation is based on the number of neighbors of each point within a certain distance in different subspaces (Z, XZ, YZ). This method is particularly useful for high-dimensional data where histogram-based methods fail.\n\n\n\n\n\n","category":"method"},{"location":"#InformationTheory.jl","page":"Home","title":"InformationTheory.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"InformationTheory.jlは，情報理論における様々な尺度を計算するためのJuliaパッケージです．","category":"page"},{"location":"","page":"Home","title":"Home","text":"このパッケージでは，現在以下の機能を提供しています．","category":"page"},{"location":"","page":"Home","title":"Home","text":"シャノンエントロピー：事象の不確かさを定量化します．\n相互情報量：2つの変数が共有する情報量を測定します．\nKL情報量：2つの確率分布の「キョリ」を測定します．\n条件付き相互情報量：ある変数の影響を除く，2つの変数が共有する情報量を測定します．","category":"page"},{"location":"","page":"Home","title":"Home","text":"詳細については，以下の各ドキュメントページをご覧ください．","category":"page"},{"location":"","page":"Home","title":"Home","text":"シャノンエントロピー\n相互情報量\nKL情報量\n条件付き相互情報量","category":"page"},{"location":"","page":"Home","title":"Home","text":"このプロジェクトは，GitHubで開発されています．貢献やフィードバックを歓迎します．","category":"page"},{"location":"API/KLdivergence/#KL-divergence","page":"KL Divergence","title":"KL divergence","text":"","category":"section"},{"location":"API/KLdivergence/#InformationTheory.KLdivergence.Hist","page":"KL Divergence","title":"InformationTheory.KLdivergence.Hist","text":"Hist(; bins_x::Tuple = (-1,), bins_y::Tuple = (-1,))\n\nA method for calculating KL divergence using histograms.\n\nFields\n\nbins_x::Tuple: A tuple specifying the binning strategy for the histogram of the first distribution (P).                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\nbins_y::Tuple: A tuple specifying the binning strategy for the histogram of the second distribution (Q).                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/KLdivergence/#InformationTheory.KLdivergence.KLdivergenceMethod","page":"KL Divergence","title":"InformationTheory.KLdivergence.KLdivergenceMethod","text":"KLdivergenceMethod\n\nAn abstract type for different methods of calculating KL divergence.\n\n\n\n\n\n","category":"type"},{"location":"API/KLdivergence/#InformationTheory.KLdivergence.kNN","page":"KL Divergence","title":"InformationTheory.KLdivergence.kNN","text":"kNN(; k::Int = 5)\n\nA method for calculating KL divergence using a k-nearest neighbors (k-NN) based estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/KLdivergence/#InformationTheory.KLdivergence.kldiv-Tuple{InformationTheory.KLdivergence.Hist, Tuple, Tuple}","page":"KL Divergence","title":"InformationTheory.KLdivergence.kldiv","text":"kldiv(method::Hist, x::Tuple, y::Tuple)\n\nCalculates the KL divergence between two distributions, P and Q, represented by data x and y, using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based KL divergence calculation method.\nx::Tuple: A tuple of vectors representing the data for the first distribution (P). Each vector is a dimension.\ny::Tuple: A tuple of vectors representing the data for the second distribution (Q). Each vector is a dimension.\n\nReturns\n\nD::Float64: The calculated KL divergence D(P||Q).\n\nDetails\n\nThe function fits histograms to the data x and y to approximate their probability density functions (PDFs), p(x) and q(y). The KL divergence is then calculated by integrating p(x) * log(p(x) / q(y)) over the domain.\n\n\n\n\n\n","category":"method"},{"location":"API/KLdivergence/#InformationTheory.KLdivergence.kldiv-Tuple{InformationTheory.KLdivergence.kNN, Tuple, Tuple}","page":"KL Divergence","title":"InformationTheory.KLdivergence.kldiv","text":"kldiv(method::kNN, x::Tuple, y::Tuple)\n\nCalculates the KL divergence between two distributions, P and Q, represented by data x and y, using a k-NN based method.\n\nArguments\n\nmethod::kNN: The k-NN based KL divergence calculation method.\nx::Tuple: A tuple of vectors representing the data for the first distribution (P).\ny::Tuple: A tuple of vectors representing the data for the second distribution (Q).\n\nReturns\n\nD::Float64: The calculated KL divergence D(P||Q).\n\nDetails\n\nThis function uses a non-parametric method to estimate KL divergence based on the distances to the k-nearest neighbors in the data x and y. It is particularly useful for high-dimensional data.\n\n\n\n\n\n","category":"method"},{"location":"Tutorial/KLdivergence/#KL-divergence","page":"KL Divergence","title":"KL divergence","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"このページでは，カルバック・ライブラー情報量 (Kullback-Leibler divergence) の計算関数kldivについて説明します．","category":"page"},{"location":"Tutorial/KLdivergence/#カルバック・ライブラー情報量","page":"KL Divergence","title":"カルバック・ライブラー情報量","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"2つの確率分布PとQに対するカルバック・ライブラー情報量","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"    D_KL(PQ) = int_-infty^infty p(x) ln fracp(x)q(x)  mathrmdx","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"です．","category":"page"},{"location":"Tutorial/KLdivergence/#ヒストグラムを使った計算例","page":"KL Divergence","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"以下は，ヒストグラム推定を使った計算例です．","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000) # P\ny = randn(10000) # Q\n\n# ヒストグラム推定を使うことを指定\nest = KLdivergence.Hist()\n\n# KLダイバージェンスの推定\nkl = kldiv(est, x, y)\n\n# 結果の出力\nprintln(kl)","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"ヒストグラムを使用する際には，bins_xとbins_yを指定できます．","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"est = KLdivergence.Hist(bins_x = (-5:0.25:5,), bins_y = (-5:0.25:5,))","category":"page"},{"location":"Tutorial/KLdivergence/#k-近傍法を使った計算例","page":"KL Divergence","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"k-近傍法は，Wang et. al.(2009)が提案した手法に基づいて計算されます． 論文中ではいくつかの手法を提案していますが，ここでは","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"    D_KL(PQ) = frac1N sum_i=1^N leftpsi(ell_i) - psi(k_i)right + ln fracM-1N","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"で計算します． dは次元，NはPのデータ数，MはQのデータ数です． ell_iはPのi番目の点からPのk番目に近い点までの距離，k_iはPのi番目の点からQのk番目に近い点までの距離を表します． この距離は，NearestNeighbors.jlのkd木を使用して高速に推定されます． 距離推定には論文で提示されている通り，Chebyshev距離を用いています．","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx = randn(10000) # P\ny = randn(10000) # Q\n\n# KSG推定を使うことを指定\nest = KLdivergence.kNN()\n\n# KLダイバージェンスの推定\nkl = kldiv(est, x, y)\n\n# 結果の出力\nprintln(kl)","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"k-近傍法では，kを指定できます．","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"est = KLdivergence.kNN(k = 10)","category":"page"},{"location":"Tutorial/KLdivergence/#高次元のKL情報量","page":"KL Divergence","title":"高次元のKL情報量","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"同じ関数を利用して，より高次元の確率分布に対するKLダイバージェンス","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"    D_KL(P(X_1Y_1Z_1dots)Q(X_2Y_2Z_2dots))","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"も推定可能です．","category":"page"},{"location":"Tutorial/KLdivergence/#ヒストグラムを使った計算例-2","page":"KL Divergence","title":"ヒストグラムを使った計算例","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"ここでは，D_KL(P(XY)Q(XY))を推定する例を示します．","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\n\n# ヒストグラム推定を使うことを指定\nest = KLdivergence.Hist()\n\n# KLダイバージェンスの推定\nkl = kldiv(est, (x1, x2), (y1, y2))\n\n# 結果の出力\nprintln(kl)","category":"page"},{"location":"Tutorial/KLdivergence/#k-近傍法を使った計算例-2","page":"KL Divergence","title":"k-近傍法を使った計算例","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"using InformationTheory\n\n# 正規分布に基づくランダムデータを生成\nx1 = randn(10000)\nx2 = randn(10000)\ny1 = randn(10000)\ny2 = randn(10000)\n\n# KSG推定を使うことを指定\nest = KLdivergence.kNN()\n\n# KLダイバージェンスの推定\nkl = kldiv(est, (x1, x2), (y1, y2))\n\n# 結果の出力\nprintln(kl)","category":"page"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"次元の数には制限ありませんが，高次元になるほど推定精度は下がり，推定時間も伸びます．","category":"page"},{"location":"Tutorial/KLdivergence/#参考文献","page":"KL Divergence","title":"参考文献","text":"","category":"section"},{"location":"Tutorial/KLdivergence/","page":"KL Divergence","title":"KL Divergence","text":"Wang Q. , Kulkarni S. R., Verdu S., Divergence Estimation for Multidimensional Densities Via  k -Nearest-Neighbor Distances, in IEEE Transactions on Information Theory, Vol. 55, No. 5 (2009), pp. 2392-2405","category":"page"}]
}

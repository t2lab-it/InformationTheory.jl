var documenterSearchIndex = {"docs":
[{"location":"API/ShannonEntropy/#Shannon-Entropy","page":"Shannon Entropy","title":"Shannon Entropy","text":"","category":"section"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.Hist","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.Hist","text":"Hist(; bins::Tuple = (-1,))\n\nA method for calculating Shannon entropy using histograms.\n\nFields\n\nbins::Tuple: A tuple specifying the binning strategy for the histogram.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.KSG","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.KSG","text":"KSG(; k::Int = 5)\n\nA method for calculating Shannon entropy using the Kozachenko-Leonenko (KSG) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.ShannonEntropyMethod","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.ShannonEntropyMethod","text":"ShannonEntropyMethod\n\nAn abstract type for different methods of calculating Shannon entropy.\n\n\n\n\n\n","category":"type"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.Hist, Vararg{AbstractVector}}","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::Hist, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function first fits a histogram to the data x. The probability density function (PDF) is then approximated from the histogram. Finally, the Shannon entropy is calculated by integrating -p(x) * log(p(x)) over the domain of x, where p(x) is the PDF.\n\n\n\n\n\n","category":"method"},{"location":"API/ShannonEntropy/#InformationTheory.ShannonEntropy.shannon-Tuple{InformationTheory.ShannonEntropy.KSG, Vararg{AbstractVector}}","page":"Shannon Entropy","title":"InformationTheory.ShannonEntropy.shannon","text":"shannon(method::KSG, x::AbstractVector...)\n\nCalculates the Shannon entropy of a set of variables x using the KSG estimator.\n\nArguments\n\nmethod::KSG: The KSG Shannon entropy calculation method.\nx::AbstractVector...: One or more vectors representing the data for which to calculate the entropy. Each vector is a dimension of the data.\n\nReturns\n\nH::Float64: The calculated Shannon entropy.\n\nDetails\n\nThe function uses the k-nearest neighbors (k-NN) algorithm to estimate the probability density function (PDF) of the data. The Shannon entropy is then calculated based on the distances to the k-th nearest neighbors. This method is particularly useful for high-dimensional data.\n\n\n\n\n\n","category":"method"},{"location":"API/MutualInformation/#Mutual-Information","page":"Mutual Information","title":"Mutual Information","text":"","category":"section"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.Hist","page":"Mutual Information","title":"InformationTheory.MutualInformation.Hist","text":"Hist(; bins_x::Tuple = (-1,), bins_y::Tuple = (-1,))\n\nA method for calculating mutual information using histograms.\n\nFields\n\nbins_x::Tuple: A tuple specifying the binning strategy for the histogram of x.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\nbins_y::Tuple: A tuple specifying the binning strategy for the histogram of y.                If (-1,), the binning is determined automatically by StatsBase.fit.                Otherwise, a tuple of bin edges for each dimension should be provided.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.KSG","page":"Mutual Information","title":"InformationTheory.MutualInformation.KSG","text":"KSG(; k::Int = 5)\n\nA method for calculating mutual information using the Kozachenko-Leonenko-Granger (KSG) estimator.\n\nFields\n\nk::Int: The number of nearest neighbors to consider for each point.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.MutualInformationMethod","page":"Mutual Information","title":"InformationTheory.MutualInformation.MutualInformationMethod","text":"MutualInformationMethod\n\nAn abstract type for different methods of calculating mutual information.\n\n\n\n\n\n","category":"type"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.mutual-Tuple{InformationTheory.MutualInformation.Hist, Tuple, Tuple}","page":"Mutual Information","title":"InformationTheory.MutualInformation.mutual","text":"mutual(method::Hist, x::Tuple, y::Tuple)\n\nCalculates the mutual information between two sets of variables x and y using a histogram-based method.\n\nArguments\n\nmethod::Hist: The histogram-based mutual information calculation method.\nx::Tuple: A tuple of vectors representing the data for the first variable.\ny::Tuple: A tuple of vectors representing the data for the second variable.\n\nReturns\n\nI::Float64: The calculated mutual information.\n\nDetails\n\nThe function first fits a histogram to the joint data (x, y). The probability density functions (PDFs) p(x,y), p(x), and p(y) are then approximated from the histogram. Finally, the mutual information is calculated by integrating p(x,y) * log(p(x,y) / (p(x) * p(y))) over the domain of x and y.\n\n\n\n\n\n","category":"method"},{"location":"API/MutualInformation/#InformationTheory.MutualInformation.mutual-Tuple{InformationTheory.MutualInformation.KSG, Tuple, Tuple}","page":"Mutual Information","title":"InformationTheory.MutualInformation.mutual","text":"mutual(method::KSG, x::Tuple, y::Tuple)\n\nCalculates the mutual information between two sets of variables x and y using the KSG estimator.\n\nArguments\n\nmethod::KSG: The KSG mutual information calculation method.\nx::Tuple: A tuple of vectors representing the data for the first variable.\ny::Tuple: A tuple of vectors representing the data for the second variable.\n\nReturns\n\nI::Float64: The calculated mutual information.\n\nDetails\n\nThe function uses the k-nearest neighbors (k-NN) algorithm to estimate the mutual information. This method is particularly useful for high-dimensional data. It is based on the work of Kraskov, Stögbauer, and Grassberger (2004).\n\n\n\n\n\n","category":"method"},{"location":"#InformationTheory.jl","page":"Home","title":"InformationTheory.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"InformationTheory.jlは，情報理論における様々な尺度を計算するためのJuliaパッケージです．","category":"page"},{"location":"","page":"Home","title":"Home","text":"このパッケージでは，現在以下の機能を提供しています．","category":"page"},{"location":"","page":"Home","title":"Home","text":"シャノンエントロピー：事象の不確かさを定量化します．\n相互情報量：2つの変数が共有する情報量を測定します．","category":"page"},{"location":"","page":"Home","title":"Home","text":"詳細については，以下の各ドキュメントページをご覧ください．","category":"page"},{"location":"","page":"Home","title":"Home","text":"シャノンエントロピー\n相互情報量","category":"page"},{"location":"","page":"Home","title":"Home","text":"このプロジェクトは，GitHubで開発されています．貢献やフィードバックを歓迎します．","category":"page"}]
}
